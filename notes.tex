\documentclass{article}

\usepackage{amsfonts}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[margin=1in]{geometry}
\usepackage[small]{caption}
\usepackage{setspace}
\usepackage{hyperref}

\usepackage{biblatex}
\addbibresource{bibliography.bib}

\usepackage[shortlabels]{enumitem}

\usepackage[nottoc,notlot,notlof]{tocbibind}


\newtheorem{example}{Example}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]

\onehalfspacing

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var} 
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{Rank}

\newcommand{\ta}{\tau^{t}}
\newcommand{\hta}{\hat \tau^{t}}
\newcommand{\tb}{\tau_{b}^{t}}
\newcommand{\htb}{\hat \tau_{b}^{t}}
\newcommand{\he}{\hat{\epsilon}_{i}}
\newcommand{\Yi}{Y_{i}^{obs}}
\newcommand{\Yb}{Y_{i}^{*}}
\newcommand{\hYi}{\hat{Y_{i}(0)}}
\newcommand{\us}{u_{i}^{*}}

\begin{document}

\title{Kevin's Notes on Matching Estimators for Nik}
\author{Kevin D. Duncan}
\maketitle

\begin{table}[h!]
\title{Proposed Procedure}
\begin{enumerate}
\item Estimate $\hta$
\item Get residuals $\he = \Yi-\hYi-\hta$ for each treated observation $i$. (you constantly forget the hat on the epsilons, its important to denote that its a sample residual and not from the population!)
\item For each $i$, draw $\us = \bigg\{ \begin{array}{cc} 1 & \text{ with probability } 1/2 \\ -1 & \text{with probability} 1/2 \end{array}$
\item Create $N_{1}$ bootstrap outcomes, $\Yb = \hYi +\hta + \us\he$
\item Estimate $\htb$ on the bootstrap sample $Z = \{\Yb, W, X\}$
\item Repeat the steps 3-5 for $b = 1,...,B$ bootstraps, and estimate $\hat \tau$ as the sample variance of $\hat \tau_{b}$ over the $b$ bootstraps.
\end{enumerate}
\label{boot}
\end{table}

\section{A Brief Rederivation of the ATET}

Some Assumptions from \cite{OnR:16} and \cite{AnI:08}

\begin{assumption}\label{ATETasmp}

\begin{enumerate}
\item Conditional on $W_{i} = w$ the sample consistss of independent draws from $Y,X \mid W = w$ for $w \in \{0,1\}$. For some $r \leq 1,\ N_{1}^{r}/N_{0} \to \theta \in (0,\infty)$
\item $X$ is continuously distributed on compact and convex support $X \subset R^{k}$. The density of $X$ is bounded and bounded away from zero on $\mathbb{X}$.
\item $W$ is independent of $Y_{i}(0)$ conditional on $X = x$ for almost every $x$. There exists a positive constant $c$ such that $P(W = 1 \mid X= x) \leq 1-c$ for almost every $x$
\item For $w = 0,1$, $\mu(w,x)$ and $\sigma^{2}(w,x)$ are Lipschitz in $\mathbb{X}$, $\sigma^{2}(w,x)$ is bounded away from zero on $\mathbb{X}$, and $\E[Y^{4} \mid W=w, X=x]$ is bounded uniformly on $\mathbb{X}$.
\end{enumerate}

\end{assumption}

For each individual we can write the Average Treatment Effect on the Treated (ATET) as,\footnote{The following notation is a bastard mix from \cite{InR:15} pp 415-416 merged with notation from \cite{AnI:08}- sorry if its not perfectly clear.}

\begin{align*} \hat \tau_{i}^{t} &= Y_{i}^{obs}-Y_{m_{i}^{C}}^{obs} = Y_{i}(1)-Y_{m_{i}^{c}}(0) \end{align*}

When the matching is perfect both units of this pair have covariate values equal to that of the matched unit, e.g. $X_{i} = X_{m_{i}^{c}}$, with inexact matching, this isn't the case and $X_{i} \neq X_{m_{i}^{c}}$. Taking expectations over the population, define

$$\mu(0,x) = \E[Y_{i}(0) \mid X_{i} = x]\quad \mu(1,x) = \E[Y_{i}(1) \mid X_{i} = x]$$

Then taking expectations,
$$\E(Y_{i}^{obs}-Y_{m_{i}^{c}} \mid W_{i} = 1, X_{i} = X_{m_{i}^{c}} = x) = \E(Y_{i}^{obs}-Y_{m_{i}^{c}} \mid X_{i} = X_{m_{i}^{c}} = x) = \mu(1,x) - \mu(0,x) = \tau(x)$$

Now, let $M$ be a bandwidth such that for every $j \in \{1,...,N_{c}\}$ such that $j$ is "closer" to $i$ than $M$, then, $j \in J_{M}(i)$, being the set of matched pairs. Then, when we have $X_{i} \neq X_{m_{i}^{c}}$

\begin{align*} \E_{sp}(Y_{i}^{obs}-Y_{m_{i}^{c}} \mid W_{i} = 1, X_{i}, X_{m_{i}^{c}}) &= \E_{sp}(Y_{i}^{obs}-Y_{m_{i}^{c}} \mid X_{i}, X_{m_{i}^{c}}) = \mu(1,X_{i}) - \mu(0,X_{m_{i}^{c}}) \\
&\quad= \tau(x)+ \mu(0,X_{i}) -\mu(0,X_{m_{i}^{c}}) \end{align*}

Thus we see in finite samples, when our matching isn't perfect, that there will be a residual bias term that differs from the ATET. \cite{AnI:06} show that the bias term is stochastically bounded, in particular for the ATET it is $O_{p}(N^{-r/k})$, such that if $k > 2r$ the bias term maw disrupt the convergence in distribution. Regardless, for fixed $n$, for finite samples by summing across treated individuals,

\begin{align*} \hat \tau &= N_{1}^{-1}\sum_{W_{i} = 1}\hat \tau_{i}^{t} =\tau+N_{1}^{-1}\sum_{W_{i} = 1}\mu(0,X_{i}) -\mu(0,X_{m_{i}^{c}}) \\
&\quad= \tau+N_{1}^{-1}\sum_{i}^{N}W_{i}\left(M^{-1}\sum_{j \in J_{M}(i)}(\mu(0,X_{i}) - \mu(0,X_{j}))\right)
\end{align*}

Now define, 

$$B_{n}^{t} = N_{1}^{-1}\sum_{i}^{N}W_{i}\left(M^{-1}\sum_{j \in J_{M}(i)}(\mu(0,X_{i}) - \mu(0,X_{j}))\right)$$

Then, we have no reason in finite samples to assume that 
$$\E(B_{n}^{t})=0$$
Even when the expectation holds over the population.

The current proposed bootstrap outlined in Table \ref{boot} estimates $\hat \tau$, however, \cite{OnR:16} and \cite{AnI:06} both calculate $\tilde{\tau}^{t} = \hat \tau^{t}-B_{n}^{t}$. \cite{AnI:06} show that this bias term is $O_{p}(N_{1}^{-r/k})$. As a result, $\sqrt{N_{1}}B_{n}^{T}$ is not $O_{p}(1)$ in general, and if $k$ is large enough, the asymptotic distribution of $\sqrt{N_{1}}(\hta-\ta)$ is dominated by the bias term and the matching estimator in general is not $\sqrt{N_{1}}$-consistent. In the case where $k = r = 1$, then we get that $\sqrt{N_{1}}(\hta -\ta)$ will be asymptotically normal. But under Assumptions \ref{ATETasmp} we've tried to generalize this requirement, and thus (generally) require a bias corrected estimator for $\hta$. Naturally this suggestsk,

$$\sqrt{(\hat \tau^{t}-B_{n}^{t}-\tau)}/\sigma_{n}^{t} \Rightarrow N(0,1)$$

But now note that this is NOT what the proposed estimator is currently doing. You are not taking the bias corrected form, and estimates still include the bias term for finite samples. The large $n$ approximations required to make $B_{n}^{t}$ be $o(1)$ aren't present in your simulations, and repeatedly sampling from the data generating process doesn't fix this as far as I can see.

Instead, let us assume we have a consistent nonparametric estimators $\hat \mu(1,x) \to^{p} \mu(1,x)$ and $\hat \mu(0,x) \to^{p} \mu(0,x))$ for all $x$. Then, the natural estimator that arises is,

\begin{align*}\tilde \tau^{t} &= \hat\tau^{t}-\hat B_{n}^{t} \\
&= N_{1}^{-1}\sum_{W_{i} = 1}\left((Y_{i}^{obs} -M^{-1}\sum_{j \in J_{M}(i)}Y_{j}^{obs})-\left(M^{-1}\sum_{j \in J_{M}(i)}(\hat \mu(0,X_{i}) - \hat \mu(0,X_{j}))\right)\right) \\
&= N_{1}^{-1}\sum_{W_{i} = 1}\left((Y_{i}^{obs}-\hat \mu(0,X_{i}) -M^{-1}\sum_{j \in J_{M}(i)}Y_{j}^{obs}-\hat \mu(0,X_{j}))\right) \\
&= N_{1}^{-1}\sum_{W_{i} = 1}\left((Y_{i}^{obs}-\hat \mu(1,X_{i}) -M^{-1}\sum_{j \in J_{M}(i)}(Y_{j}^{obs}-\hat \mu(0,X_{j}))+\hat \mu(1,X_{i})-\hat \mu(0,X_{i})\right)
\end{align*}

Moreover, for later notation, we can define $K_{m}(i) = \sum_{l = 1}^{n}I\{i \in J_{m}(l)\}$ to be the number of times a particular observation appears in the match. Then,

\begin{align*} \tilde \tau^{t} &= N_{1}^{-1}\sum_{W_{i} = 1}\left((Y_{i}^{obs}-\hat \mu(1,X_{i}) -M^{-1}\sum_{j \in J_{M}(i)}(Y_{j}^{obs}-\hat \mu(0,X_{j}))+\hat \mu(1,X_{i})-\hat \mu(0,X_{i})\right) \\
&= N_{1}^{-1}\sum_{i=1}^{N}(W_{i}(\hat e_{i}+\hat \epsilon_{i})+(1-W_{i})M^{-1}K_{m}(i)\hat e_{i})\eta_{i}^{*}
\end{align*}

This expression is the same as in \cite{OnR:16} equation (6) (from their working paper, whose notation I find much easier to read). Then define $\hat e_{i} = Y_{i}^{obs}-\hat \mu(W_{i},X_{i})$, and $\epsilon_{i} = \hat \mu(1,X_{i})-\hat \mu(0,X_{i})$, such that we get

$$\tilde \tau^{t} = N_{1}^{-1}\sum_{W_{i} = 1}\left((\hat e_{i}-M^{-1}\sum_{j}\hat e_{j})+\epsilon_{i}\right)$$

\cite{OnR:16} then draw a an iid sequence $\{\eta_{i}^{*}\}_{i=1}^{N}$ such that $\E[\eta_{i} \mid Y, X< W] = 0,\ \E[\eta_{i}^{2} \mid Y, X< W] = 1,\ \E[\eta_{i}^{4} \mid Y, X< W] < \infty$ (e.g. \cite{Mam:92}) for each individual in the sample and then applies it to the above form to generate a wild bootstrap.

Your bootstrap estimator still might have the right variance, and I'll try to help with that below.

\section{Variance}

\begin{assumption}
\begin{enumerate}
\item The marginal distribution of $X$ is uniform on the interval $[0,1]$
\item The ratio of treated and control units is $N_{1}/N_{0} = \alpha$ for some $\alpha > 0$
\item The propensity score $e(x) = P(W_{i} = 1 \mid X_{i} = x)$ is constant
\item The distribution of $Y_{i}(1)$ is degenerate with $P(Y_{i}(1) = \tau) = 1$ and the conditional distribution of $Y_{i}(0)$ given $X_{i} = x$ is $N(0,1)$
\end{enumerate}
\end{assumption}

Under this framework we know that

\begin{align} \hat \tau^{t} &= N_{1}^{-1}\sum_{W_{i} = 1}Y_{i}^{obs}-\sum_{W_{i} = 0}K_{i}Y_{i} \\
&= \tau -N_{1}^{-1}\sum_{W_{i} = 0}K_{i}Y_{i} \end{align}

Conditioning on $X,W$ we know that $\E(Y_{i}(0) \mid W_{i} = 0, X) = 0$ and has conditional variance 1. And that $\E(Y_{i}(1) \mid W_{i} = 1, X) = \tau$. Then the estimatiion errors are,

\begin{align*} \he &= \Yi - \hYi-\hta \\
&= \Yi - \hYi - (N_{1}^{-1}\sum_{i}(W_{i}-(1-W_{i})K_{i})Y_{i} \\
&= \Yi - \hYi - N_{1}^{-1}\sum_{W_{i} = 1}\Yi+N_{1}^{-1}\sum_{W_{i}=0}K_{i})Y_{i}\end{align*}

In your current draft you have a minus on this last term. Am I missing something? Then,

\begin{align*}\E(\he \mid X,W) &= \E(\Yi - \hYi - N_{1}^{-1}\sum_{W_{i} =
1)}\Yi+N_{1}^{-1}\sum_{W_{i}=0}K_{i}Y_{i}) \\ &= \tau-0-\tau = 0\end{align*}

Note that under more general conditions this is not true, and the errors are no longer mean zero! Regardless, as a result we get,

\begin{align*}\E(\he^{2} \mid X,W) &= \E((\Yi - \hYi - N_{1}^{-1}\sum_{W_{i} = 1)}\Yi+N_{1}^{-1}\sum_{W_{i}=0}K_{i})Y_{i})^{2}\mid X,W) \\
&= \E((\ta-\hYi-\ta+N_{1}^{-1}\sum_{W_{i}=0}K_{i}Y_{i})^{2}\mid X,W)\\
&= \E((-\hYi+N_{1}^{-1}\sum_{W_{i}=0}K_{i}Y_{i})^{2}\mid X,W) \\
&= \E((N_{1}^{-1}\sum_{W_{i}=0}K_{i}Y_{i}-M^{-1}\sum_{j \in J_{m}(i)}Y_{j}(0))^{2} \mid X,W)\end{align*}

Since we know $Y_{i}(1) = \tau^{t}$, and $Y_{i}(0)$ is iid with mean 0 and variance 1. Moreover, by conditioning on $X,W$ we know $K_{i}$, so the only stochastic element remains $Y_{i}$. Therefore,

\begin{align*}
\Var(\he^{2} \mid X,W) &= \E(((N_{1}^{-1}\sum_{W_{i}=0}K_{i}Y_{i})^{2}-2(N_{1}^{-1}\sum_{W_{i}=0}K_{i}Y_{i})M^{-1}\sum_{j \in J_{m}(i)}Y_{j}(0))^{2}+(M^{-1}\sum_{j \in J_{m}(i)}Y_{j}(0))^{2} \mid X,W) \\
&= N_{1}^{-2}\sum_{W_{i}=0}K_{i}^{2}-2(N_{1}M)^{-1}\sum_{j \in J_{m}(i)}K_{j}+M^{-1}
\end{align*}

We can still be constructive about the asymptotic distribution of (now pulling from your pdf on the github) about $\hat \tau_{b}$.  Then, as $\us$ is iid with variance 1

\begin{align*} \Var^{*}[\hat \tau_{b}^{t} \mid X,W] &= N_{1}^{-2}\sum_{i}^{N_{1}}\E(\he^{2} \mid X,W) \\
&= N_{1}^{-2}\sum_{W_{i}=1}\left(N_{1}^{-2}\sum_{W_{i}=0}K_{i}^{2}-2(N_{1}M)^{-1}\sum_{j \in J_{m}(i)}K_{j}+M^{-1}\right) \\
&= N_{1}^{-3}\sum_{W_{i}=0}K_{i}^{2}-2N_{1}^{-3}M^{-1}\sum_{W_{i}=1}\sum_{j \in J_{m}(i)}K_{j}+(MN_{1})^{-1} \\
&=N_{1}^{-3}\sum_{W_{i}=0}K_{i}^{2}+(MN_{1})^{-1}(1-2N_{1}^{-2}\sum_{W_{i}=1}\sum_{j \in J_{m}(i)}K_{j}) \\
&= N_{1}^{-3}\sum_{W_{i}=0}K_{i}^{2}+(MN_{1})^{-1}(1-2N_{1}^{-2}\sum_{i}^{N_{0}}K_{i})\end{align*}

This feels wrong to me, but I'm still leaving it here. It appears as if the major problem is the construction of the errors $\he$ for $i = 1,...,N_{1}$. We can compare this to the conditional variance of $\hta$,

$$\Var(\hta \mid X,W) = \E(N_{1}^{-1}\sum_{i}K_{i}Y_{i}) = N_{1}^{-2}\sum_{i}K_{i}^{2}$$

Under this framework, we never have that the variances of the two estimators converge. \cite{OnR:16} do a wild bootstrap procedure that samples individual errors from the sample means, e.g. $\he = Y_{i}-\mu(W_{i},x)$. In practice, this seems equivalent of multiplying $Y_{i}^{*} = \us Y_{i}^{obs}$ for $i = 1,...,N$

$$\Var(\sqrt{N_{1}}T_{n} \mid Y,X,W) = $$



\printbibliography


\end{document}